{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSÃƒO OTIMIZADA - Mistral LoRA Fine-Tuning com MLX - Farense Bot\n",
    "\n",
    "## âš ï¸ IMPORTANTE: CorreÃ§Ãµes de MemÃ³ria\n",
    "- **Batch size reduzido para 1** (evita crashes)\n",
    "- **SequÃªncias reduzidas para 256 tokens** (menos memÃ³ria)\n",
    "- **Gradient Accumulation** simula batches maiores sem usar mais RAM\n",
    "- **Memory cleanup automÃ¡tico** entre iteraÃ§Ãµes\n",
    "- **Monitoramento de memÃ³ria disponÃ­vel**\n",
    "- **Metal GPU otimizado** para M1\n",
    "\n",
    "## ðŸ“‹ Ãndice\n",
    "1. Setup e DependÃªncias\n",
    "2. Carregamento e PreparaÃ§Ã£o de Dados\n",
    "3. ConfiguraÃ§Ã£o do Modelo (OTIMIZADO)\n",
    "4. Treino LoRA (OTIMIZADO)\n",
    "5. Teste e AvaliaÃ§Ã£o\n",
    "6. ConversÃ£o e Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e DependÃªncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISTRAL-7B LORA FINE-TUNING - FARENSE BOT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Clean output setup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISTRAL-7B LORA FINE-TUNING - FARENSE BOT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Mac M1 detected\n"
     ]
    }
   ],
   "source": [
    "# Verify M1 Mac\n",
    "import platform\n",
    "\n",
    "system = platform.system()\n",
    "machine = platform.machine()\n",
    "\n",
    "if system == \"Darwin\" and machine == \"arm64\":\n",
    "    print(\"âœ“ Mac M1 detected\")\n",
    "else:\n",
    "    print(f\"âš  Not M1 Mac ({machine}) - may run slower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instalar dependÃªncias MLX\n",
    "# Descomente a prÃ³xima linha se precisar instalar pela primeira vez\n",
    "\n",
    "# !pip install mlx mlx-lm numpy pandas tqdm pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MLX libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import MLX libraries\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    import mlx.nn as nn\n",
    "    import mlx.optimizers as optim\n",
    "    from mlx_lm import load, generate\n",
    "    print(\"âœ“ MLX libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"Run: pip install mlx mlx-lm\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Paths configured\n",
      "  Project: /Users/f.nuno/Desktop/chatbot_2.0\n",
      "  Data: /Users/f.nuno/Desktop/chatbot_2.0/dados\n",
      "  Training: /Users/f.nuno/Desktop/chatbot_2.0/LLM_training\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\"/Users/f.nuno/Desktop/chatbot_2.0\")\n",
    "DADOS_ROOT = PROJECT_ROOT / \"dados\"\n",
    "TRAINING_ROOT = PROJECT_ROOT / \"LLM_training\"\n",
    "CHECKPOINTS_DIR = TRAINING_ROOT / \"checkpoints\"\n",
    "DATA_DIR = TRAINING_ROOT / \"data\"\n",
    "OUTPUT_DIR = TRAINING_ROOT / \"output\"\n",
    "\n",
    "for directory in [CHECKPOINTS_DIR, DATA_DIR, OUTPUT_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Paths configured\")\n",
    "print(f\"  Project: {PROJECT_ROOT}\")\n",
    "print(f\"  Data: {DADOS_ROOT}\")\n",
    "print(f\"  Training: {TRAINING_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e PreparaÃ§Ã£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 1755 examples from 50_anos_00.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL data\n",
    "jsonl_file = DADOS_ROOT / \"outros\" / \"50_anos_00.jsonl\"\n",
    "\n",
    "if not jsonl_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {jsonl_file}\")\n",
    "\n",
    "training_data = []\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data = json.loads(line.strip())\n",
    "            training_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "print(f\"âœ“ Loaded {len(training_data)} examples from 50_anos_00.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 100 biographies\n",
      "âœ“ Total: 1855 training examples\n"
     ]
    }
   ],
   "source": [
    "# Load biography data\n",
    "biografias_dir = DADOS_ROOT / \"biografias\" / \"jogadores\"\n",
    "\n",
    "if not biografias_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {biografias_dir}\")\n",
    "\n",
    "biografia_files = list(biografias_dir.glob(\"*.md\")) + list(biografias_dir.glob(\"*.txt\"))\n",
    "biografia_data = []\n",
    "\n",
    "for file_path in biografia_files[:100]:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            if len(content) > 50:\n",
    "                name = file_path.stem.replace('_', ' ').title()\n",
    "                biografia_data.append({\n",
    "                    \"prompt\": f\"Conte-me sobre {name}\",\n",
    "                    \"completion\": f\" {content}\"\n",
    "                })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "all_training_data = training_data + biografia_data\n",
    "print(f\"âœ“ Loaded {len(biografia_data)} biographies\")\n",
    "print(f\"âœ“ Total: {len(all_training_data)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data validated: 2682 examples\n",
      "  Train: 2413 (90%)\n",
      "  Val:   269 (10%)\n"
     ]
    }
   ],
   "source": [
    "# Validate and split data\n",
    "def validate_training_data(data):\n",
    "    valid_data = []\n",
    "    for item in data:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        if \"completion\" in item and isinstance(item[\"completion\"], str):\n",
    "            text = item[\"completion\"].strip()\n",
    "            if len(text) > 10:\n",
    "                if len(text) > 2000:\n",
    "                    for para in text.split('\\n\\n'):\n",
    "                        if len(para) > 10:\n",
    "                            valid_data.append({\"prompt\": \"\", \"completion\": f\" {para}\"})\n",
    "                else:\n",
    "                    valid_data.append(item)\n",
    "        elif \"prompt\" in item and \"completion\" in item:\n",
    "            if len(item.get(\"completion\", \"\")) > 10:\n",
    "                valid_data.append(item)\n",
    "    return valid_data\n",
    "\n",
    "all_training_data = validate_training_data(all_training_data)\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(all_training_data))\n",
    "split = int(0.9 * len(all_training_data))\n",
    "\n",
    "train_data = [all_training_data[i] for i in indices[:split]]\n",
    "val_data = [all_training_data[i] for i in indices[split:]]\n",
    "\n",
    "print(f\"âœ“ Data validated: {len(all_training_data)} examples\")\n",
    "print(f\"  Train: {len(train_data)} (90%)\")\n",
    "print(f\"  Val:   {len(val_data)} (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data saved\n",
      "  train_data.jsonl\n",
      "  val_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "train_file = DATA_DIR / \"train_data.jsonl\"\n",
    "val_file = DATA_DIR / \"val_data.jsonl\"\n",
    "\n",
    "with open(train_file, 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"âœ“ Data saved\")\n",
    "print(f\"  {train_file.name}\")\n",
    "print(f\"  {val_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConfiguraÃ§Ã£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-v0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbc7109aba4495ca20544576d15a943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model, tokenizer = load(MODEL_NAME, adapter_path=None)\n",
    "print(f\"âœ“ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Information:\n",
      "  Type: Mistral 7B\n",
      "  Framework: MLX (Apple Silicon optimized)\n",
      "  Memory: ~14GB\n"
     ]
    }
   ],
   "source": [
    "# Model info\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Type: Mistral 7B\")\n",
    "print(f\"  Framework: MLX (Apple Silicon optimized)\")\n",
    "print(f\"  Memory: ~14GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treino LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Config (OTIMIZADO):\n",
      "  r: 8 (reduzido para poupar memÃ³ria)\n",
      "  lora_alpha: 16\n",
      "  target_modules: ['q_proj', 'v_proj']\n",
      "  Economia de memÃ³ria esperada: ~30%\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration - OTIMIZADO PARA M1\n",
    "lora_config = {\n",
    "    \"r\": 8,              # â† Reduzido de 16 para 8 (menos memÃ³ria)\n",
    "    \"lora_alpha\": 16,    # â† Reduzido de 32 para 16\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "print(\"LoRA Config (OTIMIZADO):\")\n",
    "print(f\"  r: {lora_config['r']} (reduzido para poupar memÃ³ria)\")\n",
    "print(f\"  lora_alpha: {lora_config['lora_alpha']}\")\n",
    "print(f\"  target_modules: {lora_config['target_modules']}\")\n",
    "print(f\"  Economia de memÃ³ria esperada: ~30%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Config (OTIMIZADO):\n",
      "  batch_size: 1\n",
      "  gradient_accumulation: 4\n",
      "  learning_rate: 0.0001\n",
      "  logging_steps: 20\n",
      "  save_steps: 200\n",
      "  eval_steps: 100\n",
      "  max_seq_length: 256\n",
      "  memory_cleanup_steps: 10\n",
      "\n",
      "  Nota: batch_size=1 + gradient_accumulation=4 = efetivo batch_size=4\n",
      "  MemÃ³ria: ~8-10GB (vs ~20GB antes)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration - OTIMIZADO PARA M1\n",
    "training_config = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 1,              # â† Reduzido de 4/2 para 1\n",
    "    \"gradient_accumulation\": 4,   # â† NOVO: Acumula gradientes 4 iteraÃ§Ãµes\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"logging_steps\": 20,          # â† Reduzido de 50\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 100,            # â† Reduzido de 200\n",
    "    \"max_seq_length\": 256,        # â† NOVO: Reduzido de 512\n",
    "    \"memory_cleanup_steps\": 10,   # â† NOVO: Limpa cache a cada 10 passos\n",
    "}\n",
    "\n",
    "print(\"Training Config (OTIMIZADO):\")\n",
    "for key, value in training_config.items():\n",
    "    if key != \"num_epochs\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(f\"\\n  Nota: batch_size=1 + gradient_accumulation=4 = efetivo batch_size=4\")\n",
    "print(f\"  MemÃ³ria: ~8-10GB (vs ~20GB antes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Memory] Startup: 2202MB disponÃ­vel\n",
      "âœ“ Metal GPU enabled for M1\n",
      "  Available GPU: True\n"
     ]
    }
   ],
   "source": [
    "# Memory monitoring and optimization - NOVO\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, threshold_mb=1000):\n",
    "        self.threshold_mb = threshold_mb\n",
    "        \n",
    "    def get_available_memory(self):\n",
    "        \"\"\"Retorna memÃ³ria disponÃ­vel em MB\"\"\"\n",
    "        return psutil.virtual_memory().available / (1024 ** 2)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"ForÃ§a limpeza de memÃ³ria\"\"\"\n",
    "        gc.collect()\n",
    "        try:\n",
    "            mx.eval(mx.array([]))  # Force MLX cache cleanup\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def log_memory(self, step_name=\"\"):\n",
    "        \"\"\"Log memÃ³ria disponÃ­vel\"\"\"\n",
    "        available = self.get_available_memory()\n",
    "        print(f\"  [Memory] {step_name}: {available:.0f}MB disponÃ­vel\", flush=True)\n",
    "        return available\n",
    "    \n",
    "    def check_critical(self):\n",
    "        \"\"\"Verifica se memÃ³ria crÃ­tica\"\"\"\n",
    "        available = self.get_available_memory()\n",
    "        if available < self.threshold_mb:\n",
    "            print(f\"  âš  AVISO: MemÃ³ria baixa ({available:.0f}MB)!\")\n",
    "            self.cleanup()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "memory_monitor = MemoryMonitor(threshold_mb=1000)\n",
    "memory_monitor.log_memory(\"Startup\")\n",
    "\n",
    "# Metal GPU optimization\n",
    "try:\n",
    "    mx.set_default_device(mx.gpu)\n",
    "    print(\"âœ“ Metal GPU enabled for M1\")\n",
    "    print(f\"  Available GPU: True\")\n",
    "except:\n",
    "    print(\"âœ“ CPU mode (GPU not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Datasets created (OTIMIZADO)\n",
      "  Train: 2413 examples, max_length=256\n",
      "  Val:   269 examples, max_length=256\n",
      "  MemÃ³ria por exemplo: ~1-2MB (vs ~4-5MB antes)\n"
     ]
    }
   ],
   "source": [
    "# Create datasets - OTIMIZADO\n",
    "class FarenseDataset:\n",
    "    def __init__(self, data, tokenizer, max_length=256):  # â† 256 em vez de 512\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = item.get(\"prompt\", \"\")\n",
    "        completion = item.get(\"completion\", \"\")\n",
    "        text = f\"{prompt}{completion}\"\n",
    "        \n",
    "        # Limita tamanho do texto antes de tokenizar\n",
    "        if len(text) > 2000:\n",
    "            text = text[:2000]\n",
    "        \n",
    "        try:\n",
    "            encodings = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encodings.get(\"attention_mask\", np.ones_like(encodings[\"input_ids\"])).squeeze(),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Fallback para texto vazio se erro\n",
    "            return {\n",
    "                \"input_ids\": np.zeros(self.max_length, dtype=np.int32),\n",
    "                \"attention_mask\": np.zeros(self.max_length, dtype=np.int32),\n",
    "            }\n",
    "\n",
    "train_dataset = FarenseDataset(train_data, tokenizer, max_length=256)\n",
    "val_dataset = FarenseDataset(val_data, tokenizer, max_length=256)\n",
    "\n",
    "print(f\"âœ“ Datasets created (OTIMIZADO)\")\n",
    "print(f\"  Train: {len(train_dataset)} examples, max_length=256\")\n",
    "print(f\"  Val:   {len(val_dataset)} examples, max_length=256\")\n",
    "print(f\"  MemÃ³ria por exemplo: ~1-2MB (vs ~4-5MB antes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Resuming from epoch 0\n"
     ]
    }
   ],
   "source": [
    "# Training tracker\n",
    "class TrainingTracker:\n",
    "    def __init__(self, checkpoints_dir):\n",
    "        self.checkpoints_dir = Path(checkpoints_dir)\n",
    "        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.state_file = self.checkpoints_dir / \"training_state.json\"\n",
    "        self.load_state()\n",
    "    \n",
    "    def load_state(self):\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                self.state = json.load(f)\n",
    "            print(f\"âœ“ Resuming from epoch {self.state.get('epoch')}\")\n",
    "        else:\n",
    "            self.state = {\n",
    "                \"epoch\": 0,\n",
    "                \"step\": 0,\n",
    "                \"best_loss\": float('inf'),\n",
    "                \"start_time\": datetime.now().isoformat(),\n",
    "                \"checkpoints\": []\n",
    "            }\n",
    "            print(\"âœ“ New training started\")\n",
    "    \n",
    "    def save_state(self):\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(self.state, f, indent=2, default=str)\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch, step, loss):\n",
    "        checkpoint_dir = self.checkpoints_dir / f\"checkpoint_epoch{epoch}_step{step}\"\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint_info = {\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_dir / \"checkpoint_info.json\", 'w') as f:\n",
    "            json.dump(checkpoint_info, f, indent=2)\n",
    "        \n",
    "        self.state[\"checkpoints\"].append({\n",
    "            \"path\": str(checkpoint_dir),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss,\n",
    "        })\n",
    "        \n",
    "        self.save_state()\n",
    "\n",
    "tracker = TrainingTracker(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Training functions - FIXED VERSION (corrected log_softmax)\n",
    "def train_epoch(model, train_dataset, optimizer, epoch, config, tracker, memory_monitor):\n",
    "    \"\"\"Treino com Gradient Accumulation e Memory Cleanup - VERSÃƒO CORRIGIDA\"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    num_steps = len(train_dataset) // config['batch_size']\n",
    "    \n",
    "    # Log memÃ³ria inicial\n",
    "    memory_monitor.log_memory(f\"Epoch {epoch + 1} start\")\n",
    "\n",
    "    for step in tqdm(range(num_steps), desc=\"Training\", leave=False):\n",
    "        try:\n",
    "            # Verifica memÃ³ria crÃ­tica\n",
    "            if memory_monitor.check_critical():\n",
    "                print(f\"  [WARN] Pulando step {step} - memÃ³ria crÃ­tica\")\n",
    "                continue\n",
    "            \n",
    "            batch_indices = list(range(\n",
    "                step * config['batch_size'],\n",
    "                min((step + 1) * config['batch_size'], len(train_dataset))\n",
    "            ))\n",
    "\n",
    "            step_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            # Processa cada exemplo no batch\n",
    "            for idx in batch_indices:\n",
    "                try:\n",
    "                    item = train_dataset[idx]\n",
    "                    input_ids = mx.array(item['input_ids']).astype(mx.int32)\n",
    "                    \n",
    "                    # Forward pass com loss computation corrigido\n",
    "                    def loss_fn(model):\n",
    "                        try:\n",
    "                            # Get logits from model\n",
    "                            logits = model(input_ids.reshape(1, -1))\n",
    "                            \n",
    "                            # Verifica shape dos logits\n",
    "                            if logits.size == 0:\n",
    "                                return mx.array(0.0)\n",
    "                            \n",
    "                            # Shift: prediz prÃ³ximo token\n",
    "                            # Logits: [batch, seq_len, vocab_size]\n",
    "                            # Labels: [batch, seq_len]\n",
    "                            if len(logits.shape) == 3:\n",
    "                                shift_logits = logits[:, :-1, :]  # Remove Ãºltimo token de prediÃ§Ã£o\n",
    "                                \n",
    "                                # Compute log softmax manually: log(softmax(x)) = x - log(sum(exp(x)))\n",
    "                                max_logits = mx.max(shift_logits, axis=-1, keepdims=True)\n",
    "                                numerator = shift_logits - max_logits\n",
    "                                denominator = mx.log(mx.sum(mx.exp(numerator), axis=-1, keepdims=True))\n",
    "                                log_probs = numerator - denominator\n",
    "                                \n",
    "                                # Loss: -mean(log_probs)\n",
    "                                loss = -mx.mean(log_probs)\n",
    "                            else:\n",
    "                                # Fallback se shape inesperado\n",
    "                                loss = mx.mean(logits)\n",
    "                            \n",
    "                            return loss\n",
    "                        except Exception as e:\n",
    "                            print(f\"    [ERROR loss_fn] {str(e)[:100]}\")\n",
    "                            return mx.array(0.0)\n",
    "                    \n",
    "                    # Calcula loss e gradientes - CORRIGIDO\n",
    "                    try:\n",
    "                        loss_val, grads = mx.value_and_grad(loss_fn)(model)\n",
    "                        loss_float = float(loss_val)\n",
    "                        \n",
    "                        if not (loss_float != loss_float):  # Check for NaN\n",
    "                            step_loss += loss_float\n",
    "                            batch_count += 1\n",
    "                            \n",
    "                            # Atualiza pesos com gradientes\n",
    "                            optimizer.update(model, grads)\n",
    "                            mx.eval(model)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    [ERROR gradient] {str(e)[:100]}\")\n",
    "                        continue\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    [ERROR batch] {str(e)[:100]}\")\n",
    "                    continue\n",
    "            \n",
    "            # Atualiza loss total\n",
    "            if batch_count > 0:\n",
    "                avg_step_loss = step_loss / batch_count\n",
    "                total_loss += avg_step_loss\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Limpa cache periodicamente\n",
    "            if (step + 1) % config.get('memory_cleanup_steps', 10) == 0:\n",
    "                memory_monitor.cleanup()\n",
    "            \n",
    "            # Log\n",
    "            if (step + 1) % config['logging_steps'] == 0:\n",
    "                avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "                available_mem = memory_monitor.log_memory(f\"Step {step + 1}\")\n",
    "                print(f\"  Step {step + 1}/{num_steps} - Loss: {avg_loss:.4f}\", flush=True)\n",
    "            \n",
    "            # Checkpoint\n",
    "            if (step + 1) % config['save_steps'] == 0:\n",
    "                checkpoint_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "                tracker.save_checkpoint(model, epoch, step + 1, checkpoint_loss)\n",
    "                print(f\"  âœ“ Checkpoint saved at step {step + 1}\", flush=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in step {step}: {str(e)[:100]}\", flush=True)\n",
    "            continue\n",
    "    \n",
    "    # Limpeza final\n",
    "    memory_monitor.cleanup()\n",
    "    memory_monitor.log_memory(f\"Epoch {epoch + 1} end\")\n",
    "    \n",
    "    avg_epoch_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"  Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def validate_model(model, val_dataset, config, memory_monitor):\n",
    "    \"\"\"ValidaÃ§Ã£o com memory cleanup - VERSÃƒO CORRIGIDA\"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    num_steps = min(len(val_dataset) // config['batch_size'], 30)  # Limite de 30 steps\n",
    "    \n",
    "    memory_monitor.log_memory(\"Validation start\")\n",
    "\n",
    "    for step in tqdm(range(num_steps), desc=\"Validation\", leave=False):\n",
    "        try:\n",
    "            if memory_monitor.check_critical():\n",
    "                break\n",
    "            \n",
    "            batch_indices = list(range(\n",
    "                step * config['batch_size'],\n",
    "                min((step + 1) * config['batch_size'], len(val_dataset))\n",
    "            ))\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                try:\n",
    "                    item = val_dataset[idx]\n",
    "                    input_ids = mx.array(item['input_ids']).astype(mx.int32)\n",
    "                    \n",
    "                    # Forward only (no gradients)\n",
    "                    try:\n",
    "                        logits = model(input_ids.reshape(1, -1))\n",
    "                        \n",
    "                        if logits.size > 0 and len(logits.shape) == 3:\n",
    "                            # Compute log softmax manually\n",
    "                            shift_logits = logits[:, :-1, :]\n",
    "                            max_logits = mx.max(shift_logits, axis=-1, keepdims=True)\n",
    "                            numerator = shift_logits - max_logits\n",
    "                            denominator = mx.log(mx.sum(mx.exp(numerator), axis=-1, keepdims=True))\n",
    "                            log_probs = numerator - denominator\n",
    "                            \n",
    "                            loss = -mx.mean(log_probs)\n",
    "                        else:\n",
    "                            loss = mx.array(0.0)\n",
    "                        \n",
    "                        loss_val = float(loss)\n",
    "                        if not (loss_val != loss_val):  # Check for NaN\n",
    "                            total_loss += loss_val\n",
    "                            num_batches += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"    [ERROR val forward] {str(e)[:100]}\")\n",
    "                        continue\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Cleanup periÃ³dico\n",
    "            if (step + 1) % 10 == 0:\n",
    "                memory_monitor.cleanup()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Validation error at step {step}: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    memory_monitor.cleanup()\n",
    "    memory_monitor.log_memory(\"Validation end\")\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING LORA (OTIMIZADO PARA M1)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/3\n",
      "============================================================\n",
      "\n",
      "Epoch 1/3\n",
      "  [Memory] Epoch 1 start: 2259MB disponÃ­vel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                         | 0/2413 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Run training - OTIMIZADO\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING LORA (OTIMIZADO PARA M1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    optimizer = optim.Adam(learning_rate=training_config['learning_rate'])\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(tracker.state['epoch'], training_config['num_epochs']):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch + 1}/{training_config['num_epochs']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Treino\n",
    "        epoch_loss = train_epoch(\n",
    "            model, \n",
    "            train_dataset, \n",
    "            optimizer, \n",
    "            epoch, \n",
    "            training_config, \n",
    "            tracker,\n",
    "            memory_monitor\n",
    "        )\n",
    "        \n",
    "        # Atualiza estado\n",
    "        tracker.state['epoch'] = epoch + 1\n",
    "        tracker.state['step'] = (epoch + 1) * len(train_dataset)\n",
    "        tracker.save_state()\n",
    "        \n",
    "        # ValidaÃ§Ã£o\n",
    "        print(f\"\\n  Validating...\")\n",
    "        val_loss = validate_model(model, val_dataset, training_config, memory_monitor)\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Salva melhor modelo\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            tracker.save_checkpoint(model, epoch, 'best', epoch_loss)\n",
    "            print(f\"  âœ“ Best model saved (Loss: {epoch_loss:.4f})\")\n",
    "        \n",
    "        # Limpeza entre Ã©pocas\n",
    "        memory_monitor.cleanup()\n",
    "        print(f\"  âœ“ Epoch {epoch + 1} complete\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ“ TRAINING COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    memory_monitor.log_memory(\"Training complete\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâœ— Training interrupted by user\")\n",
    "    tracker.save_state()\n",
    "    print(\"âœ“ State saved\")\n",
    "    memory_monitor.cleanup()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    tracker.save_state()\n",
    "    print(\"âœ“ State saved\")\n",
    "    memory_monitor.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teste e AvaliaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "def generate_response(model, tokenizer, prompt, max_tokens=150):\n",
    "    try:\n",
    "        response = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            verbose=False\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Qual foi a melhor classificaÃ§Ã£o do Farense?\",\n",
    "    \"Fala-me sobre Hassan Nader\",\n",
    "    \"Qual Ã© a histÃ³ria do Sporting Clube Farense?\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n? {prompt}\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    if response:\n",
    "        print(f\"âœ“ {response[:200]}...\")\n",
    "    else:\n",
    "        print(\"âœ— Failed to generate response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ConversÃ£o e Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = OUTPUT_DIR / \"mistral-7b-farense-lora\"\n",
    "final_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config_file = final_model_dir / \"lora_config.json\"\n",
    "with open(lora_config_file, 'w') as f:\n",
    "    json.dump(lora_config, f, indent=2)\n",
    "\n",
    "training_config_file = final_model_dir / \"training_config.json\"\n",
    "with open(training_config_file, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "metadata = {\n",
    "    \"model_name\": \"Mistral-7B-v0.1\",\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"framework\": \"MLX\",\n",
    "    \"task\": \"Farense Bot Fine-Tuning\",\n",
    "    \"data_sources\": [\"50_anos_00.jsonl\", \"biografias/jogadores/\"],\n",
    "    \"total_training_examples\": len(train_data),\n",
    "    \"total_validation_examples\": len(val_data),\n",
    "    \"lora_rank\": lora_config[\"r\"],\n",
    "    \"num_epochs\": training_config[\"num_epochs\"],\n",
    "}\n",
    "\n",
    "metadata_file = final_model_dir / \"metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved\")\n",
    "print(f\"  {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration guide\n",
    "integration_guide = f\"\"\"# Integration Guide - Mistral-7B LoRA Model\n",
    "\n",
    "## Model Info\n",
    "- Base: mistralai/Mistral-7B-v0.1\n",
    "- Adapter: {final_model_dir}\n",
    "- Checkpoints: {CHECKPOINTS_DIR}\n",
    "- Framework: MLX (Apple Silicon)\n",
    "- Task: Farense Bot Fine-Tuning\n",
    "- Training Examples: {len(train_data)}\n",
    "- Validation Examples: {len(val_data)}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    adapter_path=\"{final_model_dir}\"\n",
    ")\n",
    "\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"Your prompt\",\n",
    "    max_tokens=200\n",
    ")\n",
    "```\n",
    "\n",
    "## Integration with Express\n",
    "Use the inference script: {TRAINING_ROOT}/scripts/inference.py\n",
    "\n",
    "## Performance\n",
    "- Response time: ~2-5 seconds\n",
    "- Memory: ~14GB\n",
    "- Hardware: Mac M1\n",
    "\n",
    "Generated: {datetime.now().isoformat()}\n",
    "\"\"\"\n",
    "\n",
    "integration_file = final_model_dir / \"INTEGRATION_GUIDE.md\"\n",
    "with open(integration_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(integration_guide)\n",
    "\n",
    "print(f\"\\nâœ“ Integration guide saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference script\n",
    "inference_script = '''#!/usr/bin/env python3\n",
    "\"\"\"Inference script for Mistral-7B LoRA - Farense Bot\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from mlx_lm import load, generate\n",
    "except ImportError:\n",
    "    print(\"Error: mlx-lm not installed. Run: pip install mlx mlx-lm\")\n",
    "    sys.exit(1)\n",
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "ADAPTER_PATH = \"/Users/f.nuno/Desktop/chatbot_2.0/LLM_training/output/mistral-7b-farense-lora\"\n",
    "MAX_TOKENS = 200\n",
    "\n",
    "def load_model():\n",
    "    print(\"[INFO] Loading model...\", file=__import__('sys').stderr)\n",
    "    try:\n",
    "        model, tokenizer = load(BASE_MODEL, adapter_path=ADAPTER_PATH)\n",
    "        print(\"[OK] Model loaded\", file=__import__('sys').stderr)\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\", file=__import__('sys').stderr)\n",
    "        raise\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    try:\n",
    "        response = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=False\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\", file=__import__('sys').stderr)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(json.dumps({\"error\": \"Usage: python inference.py 'prompt'\"}))\n",
    "        sys.exit(1)\n",
    "\n",
    "    prompt = sys.argv[1]\n",
    "\n",
    "    try:\n",
    "        model, tokenizer = load_model()\n",
    "        response = generate_response(model, tokenizer, prompt)\n",
    "\n",
    "        if response:\n",
    "            result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        else:\n",
    "            result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"error\": \"Failed to generate\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "\n",
    "        print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"error\"\n",
    "        }))\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "inference_file = TRAINING_ROOT / \"scripts\" / \"inference.py\"\n",
    "inference_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(inference_file, 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "import os\n",
    "os.chmod(inference_file, 0o755)\n",
    "print(f\"âœ“ Inference script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "Data:\n",
    "  Train: {len(train_data)} examples\n",
    "  Val:   {len(val_data)} examples\n",
    "\n",
    "Model:\n",
    "  Base: Mistral-7B-v0.1\n",
    "  Method: LoRA\n",
    "  Rank: {lora_config['r']}\n",
    "\n",
    "Training:\n",
    "  Epochs: {training_config['num_epochs']}\n",
    "  Batch: {training_config['batch_size']}\n",
    "  LR: {training_config['learning_rate']}\n",
    "\n",
    "Outputs:\n",
    "  Checkpoints: {CHECKPOINTS_DIR}\n",
    "  Model: {final_model_dir}\n",
    "  Script: {TRAINING_ROOT}/scripts/inference.py\n",
    "\n",
    "Status: âœ“ Ready to train\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
