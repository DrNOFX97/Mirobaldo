{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral LoRA Fine-Tuning com MLX - Farense Bot\n",
    "\n",
    "Este notebook treina o Mistral-7B usando LoRA com MLX no Mac M1 para melhorar as respostas sobre o Sporting Clube Farense.\n",
    "\n",
    "## âš ï¸ IMPORTANTE: Checkpoints AutomÃ¡ticos\n",
    "- Os checkpoints sÃ£o salvos **automaticamente** a cada epoch\n",
    "- Em caso de crash, o treino retoma do Ãºltimo checkpoint\n",
    "- Dados persistem em `/tmp/farense_llm_training/`\n",
    "\n",
    "## ðŸ“‹ Ãndice\n",
    "1. Setup e DependÃªncias\n",
    "2. Carregamento e PreparaÃ§Ã£o de Dados\n",
    "3. ConfiguraÃ§Ã£o do Modelo\n",
    "4. Treino LoRA\n",
    "5. Teste e AvaliaÃ§Ã£o\n",
    "6. ConversÃ£o e Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e DependÃªncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISTRAL-7B LORA FINE-TUNING - FARENSE BOT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Clean output setup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISTRAL-7B LORA FINE-TUNING - FARENSE BOT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Mac M1 detected\n"
     ]
    }
   ],
   "source": [
    "# Verify M1 Mac\n",
    "import platform\n",
    "\n",
    "system = platform.system()\n",
    "machine = platform.machine()\n",
    "\n",
    "if system == \"Darwin\" and machine == \"arm64\":\n",
    "    print(\"âœ“ Mac M1 detected\")\n",
    "else:\n",
    "    print(f\"âš  Not M1 Mac ({machine}) - may run slower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependÃªncias MLX\n",
    "# Descomente a prÃ³xima linha se precisar instalar pela primeira vez\n",
    "\n",
    "# !pip install mlx mlx-lm numpy pandas tqdm pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MLX libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import MLX libraries\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    import mlx.nn as nn\n",
    "    import mlx.optimizers as optim\n",
    "    from mlx_lm import load, generate\n",
    "    print(\"âœ“ MLX libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"Run: pip install mlx mlx-lm\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Paths configured\n",
      "  Project: /Users/f.nuno/Desktop/chatbot_2.0\n",
      "  Data: /Users/f.nuno/Desktop/chatbot_2.0/dados\n",
      "  Training: /Users/f.nuno/Desktop/chatbot_2.0/LLM_training\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\"/Users/f.nuno/Desktop/chatbot_2.0\")\n",
    "DADOS_ROOT = PROJECT_ROOT / \"dados\"\n",
    "TRAINING_ROOT = PROJECT_ROOT / \"LLM_training\"\n",
    "CHECKPOINTS_DIR = TRAINING_ROOT / \"checkpoints\"\n",
    "DATA_DIR = TRAINING_ROOT / \"data\"\n",
    "OUTPUT_DIR = TRAINING_ROOT / \"output\"\n",
    "\n",
    "for directory in [CHECKPOINTS_DIR, DATA_DIR, OUTPUT_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Paths configured\")\n",
    "print(f\"  Project: {PROJECT_ROOT}\")\n",
    "print(f\"  Data: {DADOS_ROOT}\")\n",
    "print(f\"  Training: {TRAINING_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e PreparaÃ§Ã£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 1755 examples from 50_anos_00.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL data\n",
    "jsonl_file = DADOS_ROOT / \"outros\" / \"50_anos_00.jsonl\"\n",
    "\n",
    "if not jsonl_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {jsonl_file}\")\n",
    "\n",
    "training_data = []\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data = json.loads(line.strip())\n",
    "            training_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "print(f\"âœ“ Loaded {len(training_data)} examples from 50_anos_00.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 100 biographies\n",
      "âœ“ Total: 1855 training examples\n"
     ]
    }
   ],
   "source": [
    "# Load biography data\n",
    "biografias_dir = DADOS_ROOT / \"biografias\" / \"jogadores\"\n",
    "\n",
    "if not biografias_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {biografias_dir}\")\n",
    "\n",
    "biografia_files = list(biografias_dir.glob(\"*.md\")) + list(biografias_dir.glob(\"*.txt\"))\n",
    "biografia_data = []\n",
    "\n",
    "for file_path in biografia_files[:100]:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            if len(content) > 50:\n",
    "                name = file_path.stem.replace('_', ' ').title()\n",
    "                biografia_data.append({\n",
    "                    \"prompt\": f\"Conte-me sobre {name}\",\n",
    "                    \"completion\": f\" {content}\"\n",
    "                })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "all_training_data = training_data + biografia_data\n",
    "print(f\"âœ“ Loaded {len(biografia_data)} biographies\")\n",
    "print(f\"âœ“ Total: {len(all_training_data)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data validated: 2682 examples\n",
      "  Train: 2413 (90%)\n",
      "  Val:   269 (10%)\n"
     ]
    }
   ],
   "source": [
    "# Validate and split data\n",
    "def validate_training_data(data):\n",
    "    valid_data = []\n",
    "    for item in data:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        if \"completion\" in item and isinstance(item[\"completion\"], str):\n",
    "            text = item[\"completion\"].strip()\n",
    "            if len(text) > 10:\n",
    "                if len(text) > 2000:\n",
    "                    for para in text.split('\\n\\n'):\n",
    "                        if len(para) > 10:\n",
    "                            valid_data.append({\"prompt\": \"\", \"completion\": f\" {para}\"})\n",
    "                else:\n",
    "                    valid_data.append(item)\n",
    "        elif \"prompt\" in item and \"completion\" in item:\n",
    "            if len(item.get(\"completion\", \"\")) > 10:\n",
    "                valid_data.append(item)\n",
    "    return valid_data\n",
    "\n",
    "all_training_data = validate_training_data(all_training_data)\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(all_training_data))\n",
    "split = int(0.9 * len(all_training_data))\n",
    "\n",
    "train_data = [all_training_data[i] for i in indices[:split]]\n",
    "val_data = [all_training_data[i] for i in indices[split:]]\n",
    "\n",
    "print(f\"âœ“ Data validated: {len(all_training_data)} examples\")\n",
    "print(f\"  Train: {len(train_data)} (90%)\")\n",
    "print(f\"  Val:   {len(val_data)} (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data saved\n",
      "  train_data.jsonl\n",
      "  val_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "train_file = DATA_DIR / \"train_data.jsonl\"\n",
    "val_file = DATA_DIR / \"val_data.jsonl\"\n",
    "\n",
    "with open(train_file, 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"âœ“ Data saved\")\n",
    "print(f\"  {train_file.name}\")\n",
    "print(f\"  {val_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConfiguraÃ§Ã£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-v0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8863a49347c4a5f85522aa3938cfa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model, tokenizer = load(MODEL_NAME, adapter_path=None)\n",
    "print(f\"âœ“ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Information:\n",
      "  Type: Mistral 7B\n",
      "  Framework: MLX (Apple Silicon optimized)\n",
      "  Memory: ~14GB\n"
     ]
    }
   ],
   "source": [
    "# Model info\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Type: Mistral 7B\")\n",
    "print(f\"  Framework: MLX (Apple Silicon optimized)\")\n",
    "print(f\"  Memory: ~14GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treino LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Config:\n",
      "  r: 16\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.1\n",
      "  target_modules: ['q_proj', 'v_proj']\n",
      "  bias: none\n",
      "  task_type: CAUSAL_LM\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "for key, value in lora_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Config:\n",
      "  num_epochs: 3\n",
      "  batch_size: 4\n",
      "  learning_rate: 0.0001\n",
      "  logging_steps: 50\n",
      "  save_steps: 200\n",
      "  eval_steps: 200\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 200,\n",
    "}\n",
    "\n",
    "print(\"Training Config:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Batch size adjusted to 2 (low memory)\n",
      "âœ“ Metal GPU enabled\n"
     ]
    }
   ],
   "source": [
    "# Memory optimization for M1 Mac\n",
    "available_memory = 16\n",
    "model_memory = 14\n",
    "available_for_training = available_memory - model_memory - 1\n",
    "\n",
    "if available_for_training < 2:\n",
    "    training_config['batch_size'] = 2\n",
    "    print(f\"âœ“ Batch size adjusted to 2 (low memory)\")\n",
    "\n",
    "try:\n",
    "    mx.set_default_device(mx.gpu)\n",
    "    print(\"âœ“ Metal GPU enabled\")\n",
    "except:\n",
    "    print(\"âœ“ CPU mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Datasets created\n",
      "  Train: 2413 examples\n",
      "  Val:   269 examples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "class FarenseDataset:\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = item.get(\"prompt\", \"\")\n",
    "        completion = item.get(\"completion\", \"\")\n",
    "        text = f\"{prompt}{completion}\"\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings.get(\"attention_mask\"),\n",
    "        }\n",
    "\n",
    "train_dataset = FarenseDataset(train_data, tokenizer)\n",
    "val_dataset = FarenseDataset(val_data, tokenizer)\n",
    "\n",
    "print(f\"âœ“ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Val:   {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Resuming from epoch 3\n"
     ]
    }
   ],
   "source": [
    "# Training tracker\n",
    "class TrainingTracker:\n",
    "    def __init__(self, checkpoints_dir):\n",
    "        self.checkpoints_dir = Path(checkpoints_dir)\n",
    "        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.state_file = self.checkpoints_dir / \"training_state.json\"\n",
    "        self.load_state()\n",
    "    \n",
    "    def load_state(self):\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                self.state = json.load(f)\n",
    "            print(f\"âœ“ Resuming from epoch {self.state.get('epoch')}\")\n",
    "        else:\n",
    "            self.state = {\n",
    "                \"epoch\": 0,\n",
    "                \"step\": 0,\n",
    "                \"best_loss\": float('inf'),\n",
    "                \"start_time\": datetime.now().isoformat(),\n",
    "                \"checkpoints\": []\n",
    "            }\n",
    "            print(\"âœ“ New training started\")\n",
    "    \n",
    "    def save_state(self):\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(self.state, f, indent=2, default=str)\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch, step, loss):\n",
    "        checkpoint_dir = self.checkpoints_dir / f\"checkpoint_epoch{epoch}_step{step}\"\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint_info = {\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_dir / \"checkpoint_info.json\", 'w') as f:\n",
    "            json.dump(checkpoint_info, f, indent=2)\n",
    "        \n",
    "        self.state[\"checkpoints\"].append({\n",
    "            \"path\": str(checkpoint_dir),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss,\n",
    "        })\n",
    "        \n",
    "        self.save_state()\n",
    "\n",
    "tracker = TrainingTracker(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, train_dataset, optimizer, epoch, config, tracker):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    num_steps = len(train_dataset) // config['batch_size']\n",
    "\n",
    "    for step in tqdm(range(num_steps), desc=\"Training\", leave=False):\n",
    "        try:\n",
    "            batch_indices = list(range(\n",
    "                step * config['batch_size'],\n",
    "                min((step + 1) * config['batch_size'], len(train_dataset))\n",
    "            ))\n",
    "\n",
    "            batch_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                try:\n",
    "                    item = train_dataset[idx]\n",
    "                    input_ids = mx.array(item['input_ids'])\n",
    "\n",
    "                    def model_loss(model):\n",
    "                        output = model(input_ids)\n",
    "                        return mx.mean(output)\n",
    "\n",
    "                    loss, grads = mx.value_and_grad(model_loss)(model)\n",
    "                    optimizer.update(model, grads)\n",
    "\n",
    "                    batch_loss += float(loss) if hasattr(loss, '__float__') else float(loss.item())\n",
    "                    batch_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if batch_count > 0:\n",
    "                step_loss = batch_loss / batch_count\n",
    "                total_loss += step_loss\n",
    "                num_batches += 1\n",
    "                mx.eval(model)\n",
    "\n",
    "            if (step + 1) % config['logging_steps'] == 0:\n",
    "                avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "                print(f\"  Step {step + 1}/{num_steps} - Loss: {avg_loss:.4f}\", flush=True)\n",
    "\n",
    "            if (step + 1) % config['save_steps'] == 0:\n",
    "                checkpoint_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "                tracker.save_checkpoint(model, epoch, step + 1, checkpoint_loss)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    avg_epoch_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"  Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def validate_model(model, val_dataset, config):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    num_steps = min(len(val_dataset) // config['batch_size'], 50)\n",
    "\n",
    "    for step in tqdm(range(num_steps), desc=\"Validation\", leave=False):\n",
    "        try:\n",
    "            batch_indices = list(range(\n",
    "                step * config['batch_size'],\n",
    "                min((step + 1) * config['batch_size'], len(val_dataset))\n",
    "            ))\n",
    "\n",
    "            batch_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                try:\n",
    "                    item = val_dataset[idx]\n",
    "                    input_ids = mx.array(item['input_ids'])\n",
    "                    output = model(input_ids)\n",
    "                    loss = mx.mean(output)\n",
    "\n",
    "                    batch_loss += float(loss) if hasattr(loss, '__float__') else float(loss.item())\n",
    "                    batch_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if batch_count > 0:\n",
    "                total_loss += batch_loss / batch_count\n",
    "                num_batches += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return total_loss / num_batches if num_batches > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING LORA\n",
      "============================================================\n",
      "\n",
      "âœ“ TRAINING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING LORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    optimizer = optim.Adam(learning_rate=training_config['learning_rate'])\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(tracker.state['epoch'], training_config['num_epochs']):\n",
    "        epoch_loss = train_epoch(model, train_dataset, optimizer, epoch, training_config, tracker)\n",
    "        \n",
    "        tracker.state['epoch'] = epoch + 1\n",
    "        tracker.state['step'] = (epoch + 1) * len(train_dataset)\n",
    "        tracker.save_state()\n",
    "        \n",
    "        print(f\"  Validating...\")\n",
    "        val_loss = validate_model(model, val_dataset, training_config)\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            tracker.save_checkpoint(model, epoch, 'best', epoch_loss)\n",
    "            print(f\"  âœ“ Best model saved (Loss: {epoch_loss:.4f})\")\n",
    "\n",
    "    print(\"\\nâœ“ TRAINING COMPLETE\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâœ— Training interrupted\")\n",
    "    tracker.save_state()\n",
    "    print(\"âœ“ State saved\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error: {e}\")\n",
    "    tracker.save_state()\n",
    "    print(\"âœ“ State saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teste e AvaliaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Model\n",
      "============================================================\n",
      "\n",
      "? Qual foi a melhor classificaÃ§Ã£o do Farense?\n",
      "[WARNING] Generating with a model that requires 13812 MB which is close to the maximum recommended size of 12124 MB. This can be slow. See the documentation for possible work-arounds: https://github.com/ml-explore/mlx-lm/tree/main#large-models\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "def generate_response(model, tokenizer, prompt, max_tokens=150):\n",
    "    try:\n",
    "        response = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            verbose=False\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Qual foi a melhor classificaÃ§Ã£o do Farense?\",\n",
    "    \"Fala-me sobre Hassan Nader\",\n",
    "    \"Qual Ã© a histÃ³ria do Sporting Clube Farense?\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n? {prompt}\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    if response:\n",
    "        print(f\"âœ“ {response[:200]}...\")\n",
    "    else:\n",
    "        print(\"âœ— Failed to generate response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ConversÃ£o e Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = OUTPUT_DIR / \"mistral-7b-farense-lora\"\n",
    "final_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config_file = final_model_dir / \"lora_config.json\"\n",
    "with open(lora_config_file, 'w') as f:\n",
    "    json.dump(lora_config, f, indent=2)\n",
    "\n",
    "training_config_file = final_model_dir / \"training_config.json\"\n",
    "with open(training_config_file, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "metadata = {\n",
    "    \"model_name\": \"Mistral-7B-v0.1\",\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"framework\": \"MLX\",\n",
    "    \"task\": \"Farense Bot Fine-Tuning\",\n",
    "    \"data_sources\": [\"50_anos_00.jsonl\", \"biografias/jogadores/\"],\n",
    "    \"total_training_examples\": len(train_data),\n",
    "    \"total_validation_examples\": len(val_data),\n",
    "    \"lora_rank\": lora_config[\"r\"],\n",
    "    \"num_epochs\": training_config[\"num_epochs\"],\n",
    "}\n",
    "\n",
    "metadata_file = final_model_dir / \"metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved\")\n",
    "print(f\"  {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration guide\n",
    "integration_guide = f\"\"\"# Integration Guide - Mistral-7B LoRA Model\n",
    "\n",
    "## Model Info\n",
    "- Base: mistralai/Mistral-7B-v0.1\n",
    "- Adapter: {final_model_dir}\n",
    "- Checkpoints: {CHECKPOINTS_DIR}\n",
    "- Framework: MLX (Apple Silicon)\n",
    "- Task: Farense Bot Fine-Tuning\n",
    "- Training Examples: {len(train_data)}\n",
    "- Validation Examples: {len(val_data)}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    adapter_path=\"{final_model_dir}\"\n",
    ")\n",
    "\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"Your prompt\",\n",
    "    max_tokens=200\n",
    ")\n",
    "```\n",
    "\n",
    "## Integration with Express\n",
    "Use the inference script: {TRAINING_ROOT}/scripts/inference.py\n",
    "\n",
    "## Performance\n",
    "- Response time: ~2-5 seconds\n",
    "- Memory: ~14GB\n",
    "- Hardware: Mac M1\n",
    "\n",
    "Generated: {datetime.now().isoformat()}\n",
    "\"\"\"\n",
    "\n",
    "integration_file = final_model_dir / \"INTEGRATION_GUIDE.md\"\n",
    "with open(integration_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(integration_guide)\n",
    "\n",
    "print(f\"\\nâœ“ Integration guide saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference script\n",
    "inference_script = '''#!/usr/bin/env python3\n",
    "\"\"\"Inference script for Mistral-7B LoRA - Farense Bot\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from mlx_lm import load, generate\n",
    "except ImportError:\n",
    "    print(\"Error: mlx-lm not installed. Run: pip install mlx mlx-lm\")\n",
    "    sys.exit(1)\n",
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "ADAPTER_PATH = \"/Users/f.nuno/Desktop/chatbot_2.0/LLM_training/output/mistral-7b-farense-lora\"\n",
    "MAX_TOKENS = 200\n",
    "\n",
    "def load_model():\n",
    "    print(\"[INFO] Loading model...\", file=__import__('sys').stderr)\n",
    "    try:\n",
    "        model, tokenizer = load(BASE_MODEL, adapter_path=ADAPTER_PATH)\n",
    "        print(\"[OK] Model loaded\", file=__import__('sys').stderr)\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\", file=__import__('sys').stderr)\n",
    "        raise\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    try:\n",
    "        response = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=False\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\", file=__import__('sys').stderr)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(json.dumps({\"error\": \"Usage: python inference.py 'prompt'\"}))\n",
    "        sys.exit(1)\n",
    "\n",
    "    prompt = sys.argv[1]\n",
    "\n",
    "    try:\n",
    "        model, tokenizer = load_model()\n",
    "        response = generate_response(model, tokenizer, prompt)\n",
    "\n",
    "        if response:\n",
    "            result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        else:\n",
    "            result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"error\": \"Failed to generate\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "\n",
    "        print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"error\"\n",
    "        }))\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "inference_file = TRAINING_ROOT / \"scripts\" / \"inference.py\"\n",
    "inference_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(inference_file, 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "import os\n",
    "os.chmod(inference_file, 0o755)\n",
    "print(f\"âœ“ Inference script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "Data:\n",
    "  Train: {len(train_data)} examples\n",
    "  Val:   {len(val_data)} examples\n",
    "\n",
    "Model:\n",
    "  Base: Mistral-7B-v0.1\n",
    "  Method: LoRA\n",
    "  Rank: {lora_config['r']}\n",
    "\n",
    "Training:\n",
    "  Epochs: {training_config['num_epochs']}\n",
    "  Batch: {training_config['batch_size']}\n",
    "  LR: {training_config['learning_rate']}\n",
    "\n",
    "Outputs:\n",
    "  Checkpoints: {CHECKPOINTS_DIR}\n",
    "  Model: {final_model_dir}\n",
    "  Script: {TRAINING_ROOT}/scripts/inference.py\n",
    "\n",
    "Status: âœ“ Ready to train\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
