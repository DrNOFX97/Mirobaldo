# ü§ñ LLM Training - Mistral-7B LoRA Fine-tuning

Complete setup for fine-tuning Mistral-7B with LoRA on Mac M1 for the Farense Bot.

## üìÅ Folder Structure

```
LLM_training/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ mistral_lora_training.ipynb    # Main training notebook (FIXED)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ inference.py                   # Production inference script
‚îÇ   ‚îî‚îÄ‚îÄ [future scripts]
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ train_data.jsonl              # Generated during training
‚îÇ   ‚îî‚îÄ‚îÄ val_data.jsonl                # Generated during training
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_epoch0_step200/    # Auto-saved checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ training_state.json           # Training recovery state
‚îÇ   ‚îî‚îÄ‚îÄ [more checkpoints...]
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îî‚îÄ‚îÄ mistral-7b-farense-lora/      # Final trained model + adapter
‚îÇ       ‚îú‚îÄ‚îÄ lora_config.json
‚îÇ       ‚îú‚îÄ‚îÄ training_config.json
‚îÇ       ‚îú‚îÄ‚îÄ metadata.json
‚îÇ       ‚îî‚îÄ‚îÄ [model weights]
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ README.md                      # This file
    ‚îú‚îÄ‚îÄ QUICK_START.md                # Quick reference
    ‚îú‚îÄ‚îÄ FIXES_SUMMARY.md              # Technical details of fixes
    ‚îî‚îÄ‚îÄ INTEGRATION_GUIDE.md          # How to use the trained model
```

## üöÄ Quick Start

### 1. **Open the Notebook**
```bash
cd /Users/f.nuno/Desktop/chatbot_2.0
jupyter notebook LLM_training/notebooks/mistral_lora_training.ipynb
```

### 2. **Run Training**
- Execute cells sequentially (Cells 1-21)
- Estimated time: 2-4 hours on M1 Mac
- Checkpoints auto-save every 200 steps

### 3. **Test Inference**
```bash
python3 LLM_training/scripts/inference.py "Quem foi Hassan Nader?"
```

## ‚úÖ What's Fixed in This Notebook

### Issue 1: Simulated Training
- **Before**: Loss values were fake (2.0 - epoch * 0.1)
- **After**: Real MLX gradients with actual backpropagation

### Issue 2: Hardcoded Validation
- **Before**: val_loss = 0 (not calculated)
- **After**: Real validation loss computation

### Issue 3: No Memory Optimization
- **Before**: Could crash on 16GB M1 Mac
- **After**: Auto-adjusts batch size, enables Metal GPU

## üìä Training Configuration

```python
num_epochs = 3
batch_size = 4 (auto-adjusts if needed)
learning_rate = 0.0001
save_steps = 200
eval_steps = 200
```

**Model**: Mistral-7B-v0.1 (7 billion parameters)
**Framework**: MLX (Apple Silicon optimized)
**Adapter**: LoRA (Low-Rank Adaptation)

## üíæ Data Sources

- `50_anos_00.jsonl` - Historical Farense data (1,755 examples)
- `biografias/jogadores/` - Player biographies (100 examples)
- **Total**: 2,682 training examples after validation

## üß™ Inference Script

The production-ready inference script is located at:
```
LLM_training/scripts/inference.py
```

### Usage:
```bash
python3 LLM_training/scripts/inference.py "Your prompt here"
```

### Output Format:
```json
{
  "prompt": "Your prompt here",
  "response": "Model's response...",
  "status": "success"
}
```

## üìà Training Monitoring

### Watch Live Logs
```bash
tail -f LLM_training/checkpoints/training_state.json
```

### Check Checkpoint Status
```bash
ls -lh LLM_training/checkpoints/
cat LLM_training/checkpoints/training_state.json
```

### Expected Loss Progression
- Epoch 1: ~1.69 ‚Üí 1.60
- Epoch 2: ~1.60 ‚Üí 1.50
- Epoch 3: ~1.50 ‚Üí 1.40

(Note: Actual values depend on data and initialization)

## üîÑ Resume from Checkpoint

If training is interrupted:

1. Simply run the notebook again
2. It will automatically detect the last checkpoint
3. Training resumes from where it stopped

No manual intervention needed!

## üéØ After Training

### 1. **Copy Trained Model**
```bash
cp -r LLM_training/output/mistral-7b-farense-lora /Users/f.nuno/Desktop/chatbot_2.0/models/
```

### 2. **Update Inference Path**
Update your inference script to point to the new model:
```python
ADAPTER_PATH = "/Users/f.nuno/Desktop/chatbot_2.0/models/mistral-7b-farense-lora"
```

### 3. **Integrate with Express API**
```javascript
// src/routes/llm.js
const { spawn } = require('child_process');

async function generateWithLoRA(prompt) {
  return new Promise((resolve, reject) => {
    const python = spawn('python3', ['LLM_training/scripts/inference.py', prompt]);

    let output = '';
    python.stdout.on('data', (data) => {
      output += data.toString();
    });

    python.on('close', (code) => {
      if (code === 0) resolve(JSON.parse(output));
      else reject(`Error: Exit code ${code}`);
    });
  });
}
```

## ‚ö†Ô∏è Troubleshooting

### Memory Issues
- ‚úÖ Already handled! Batch size auto-adjusts
- If still issues: Reduce batch_size to 1 in notebook

### "Model requires 13.8GB"
- ‚úÖ Warning only - will still work, just slower
- No action needed for M1 Mac

### Training Very Slow
- Check Metal GPU is enabled: Look for "GPU (Metal Performance Shaders) ativado"
- Reduce batch_size if memory is tight

### Adapter Not Found
- Make sure training completed successfully
- Check `LLM_training/output/` folder

## üìö Documentation

- **Quick Start**: `docs/QUICK_START.md`
- **Technical Details**: `docs/FIXES_SUMMARY.md`
- **Integration Guide**: `docs/INTEGRATION_GUIDE.md` (in notebook output)

## üîó Resources

- MLX Documentation: https://ml-explore.github.io/mlx/
- mlx-lm Examples: https://github.com/ml-explore/mlx-examples/tree/main/lm
- Mistral Models: https://huggingface.co/mistralai/Mistral-7B-v0.1
- LoRA Paper: https://arxiv.org/abs/2106.09685

## üìù Files Overview

### Notebooks
- `mistral_lora_training.ipynb` - Main training notebook with all fixes

### Scripts
- `inference.py` - Production inference with JSON API

### Documentation
- `QUICK_START.md` - Quick reference guide
- `FIXES_SUMMARY.md` - Detailed technical fixes
- `INTEGRATION_GUIDE.md` - Generated during training

### Auto-Generated (During Training)
- `data/train_data.jsonl` - 90% of examples
- `data/val_data.jsonl` - 10% of examples
- `checkpoints/` - Automatic checkpoint saves
- `output/mistral-7b-farense-lora/` - Final trained model

## ‚ú® Key Features

‚úÖ Real MLX training (not simulated)
‚úÖ Actual validation loss tracking
‚úÖ Automatic memory optimization
‚úÖ Checkpoint recovery system
‚úÖ Production-ready inference script
‚úÖ JSON API output format
‚úÖ Full error handling
‚úÖ M1 Mac optimized

## üéØ Next Steps

1. Review `docs/QUICK_START.md`
2. Open the notebook: `notebooks/mistral_lora_training.ipynb`
3. Run cells 1-21 sequentially
4. Monitor training progress
5. Test inference when complete
6. Integrate with your Express API

---

**Status**: ‚úÖ Ready to train!

Generated: 2025-10-28
