{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral LoRA Fine-Tuning com MLX - Farense Bot\n",
    "\n",
    "Este notebook treina o Mistral-7B usando LoRA com MLX no Mac M1 para melhorar as respostas sobre o Sporting Clube Farense.\n",
    "\n",
    "## ⚠️ IMPORTANTE: Checkpoints Automáticos\n",
    "- Os checkpoints são salvos **automaticamente** a cada epoch\n",
    "- Em caso de crash, o treino retoma do último checkpoint\n",
    "- Dados persistem em `/tmp/farense_llm_training/`\n",
    "\n",
    "## 📋 Índice\n",
    "1. Setup e Dependências\n",
    "2. Carregamento e Preparação de Dados\n",
    "3. Configuração do Modelo\n",
    "4. Treino LoRA\n",
    "5. Teste e Avaliação\n",
    "6. Conversão e Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Iniciando setup do ambiente...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se estamos no Mac M1\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "system = platform.system()\n",
    "machine = platform.machine()\n",
    "\n",
    "logger.info(f\"Sistema: {system}\")\n",
    "logger.info(f\"Arquitetura: {machine}\")\n",
    "\n",
    "if system != \"Darwin\" or machine != \"arm64\":\n",
    "    logger.warning(f\"⚠️ Este notebook é otimizado para Mac M1 (arm64). Detectado: {machine}\")\n",
    "    logger.warning(\"O treino pode ser mais lento em outras arquiteturas.\")\n",
    "else:\n",
    "    logger.info(\"✅ Mac M1 detectado - Otimizações MLX ativas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências MLX\n",
    "# Descomente a próxima linha se precisar instalar pela primeira vez\n",
    "\n",
    "# !pip install mlx mlx-lm numpy pandas tqdm pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentar importar MLX\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    import mlx.nn as nn\n",
    "    import mlx.optimizers as optim\n",
    "    from mlx.utils import tree_map, tree_flatten\n",
    "    logger.info(\"✅ MLX importado com sucesso\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"❌ Erro ao importar MLX: {e}\")\n",
    "    logger.error(\"Execute: pip install mlx mlx-lm\")\n",
    "    raise\n",
    "\n",
    "# Tentar importar mlx_lm\n",
    "try:\n",
    "    from mlx_lm import load, generate\n",
    "    logger.info(\"✅ mlx-lm importado com sucesso\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"❌ Erro ao importar mlx-lm: {e}\")\n",
    "    logger.error(\"Execute: pip install mlx-lm\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar caminhos\n",
    "PROJECT_ROOT = Path(\"/Users/f.nuno/Desktop/chatbot_2.0\")\n",
    "DADOS_ROOT = PROJECT_ROOT / \"dados\"\n",
    "TRAINING_ROOT = Path(\"/tmp/farense_llm_training\")\n",
    "CHECKPOINTS_DIR = TRAINING_ROOT / \"checkpoints\"\n",
    "OUTPUT_DIR = TRAINING_ROOT / \"output\"\n",
    "LOGS_DIR = TRAINING_ROOT / \"logs\"\n",
    "\n",
    "# Criar diretórios se não existirem\n",
    "for directory in [TRAINING_ROOT, CHECKPOINTS_DIR, OUTPUT_DIR, LOGS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"📁 Diretório criado/verificado: {directory}\")\n",
    "\n",
    "logger.info(f\"\\n📍 Raiz do Projeto: {PROJECT_ROOT}\")\n",
    "logger.info(f\"📍 Dados: {DADOS_ROOT}\")\n",
    "logger.info(f\"📍 Treino: {TRAINING_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Preparação de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados existentes em JSONL\n",
    "jsonl_file = DADOS_ROOT / \"outros\" / \"50_anos_00.jsonl\"\n",
    "\n",
    "if not jsonl_file.exists():\n",
    "    logger.error(f\"❌ Arquivo não encontrado: {jsonl_file}\")\n",
    "    raise FileNotFoundError(f\"Arquivo JSONL não encontrado: {jsonl_file}\")\n",
    "\n",
    "logger.info(f\"📄 Carregando dados de: {jsonl_file}\")\n",
    "\n",
    "training_data = []\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data = json.loads(line.strip())\n",
    "            training_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            logger.warning(f\"⚠️ Linha JSON inválida ignorada\")\n",
    "            continue\n",
    "\n",
    "logger.info(f\"✅ {len(training_data)} exemplos carregados\")\n",
    "logger.info(f\"\\n📊 Amostra dos dados:\")\n",
    "for i, item in enumerate(training_data[:3]):\n",
    "    print(f\"  {i+1}. {json.dumps(item, ensure_ascii=False, indent=2)[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados de biográfias em Markdown\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "biografias_dir = DADOS_ROOT / \"biografias\" / \"jogadores\"\n",
    "\n",
    "if not biografias_dir.exists():\n",
    "    logger.error(f\"❌ Diretório não encontrado: {biografias_dir}\")\n",
    "    raise FileNotFoundError(f\"Diretório de biográfias não encontrado: {biografias_dir}\")\n",
    "\n",
    "logger.info(f\"📚 Carregando biográfias de: {biografias_dir}\")\n",
    "\n",
    "biografia_files = list(biografias_dir.glob(\"*.md\")) + list(biografias_dir.glob(\"*.txt\"))\n",
    "logger.info(f\"📄 Encontrados {len(biografia_files)} arquivos de biografia\")\n",
    "\n",
    "biografia_data = []\n",
    "for file_path in biografia_files[:100]:  # Limitar a 100 para começar\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if len(content.strip()) > 50:  # Ignorar arquivos muito pequenos\n",
    "                # Extrair nome do arquivo\n",
    "                name = file_path.stem.replace('_', ' ').title()\n",
    "                biografia_data.append({\n",
    "                    \"prompt\": f\"Conte-me sobre {name}\",\n",
    "                    \"completion\": f\" {content}\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"⚠️ Erro ao carregar {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "logger.info(f\"✅ {len(biografia_data)} biográfias processadas\")\n",
    "\n",
    "# Combinar todos os dados\n",
    "all_training_data = training_data + biografia_data\n",
    "logger.info(f\"\\n📊 Total de exemplos de treino: {len(all_training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar e limpar dados\n",
    "def validate_training_data(data):\n",
    "    \"\"\"Valida e limpa dados de treino\"\"\"\n",
    "    valid_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Verificar se tem completion ou prompt\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "            \n",
    "        # Se tem apenas completion, usar como prompt para próxima predição\n",
    "        if \"completion\" in item and isinstance(item[\"completion\"], str):\n",
    "            text = item[\"completion\"].strip()\n",
    "            if len(text) > 10:  # Mínimo de caracteres\n",
    "                # Quebrar em chunks se muito longo\n",
    "                if len(text) > 2000:\n",
    "                    # Dividir em parágrafos\n",
    "                    paragraphs = text.split('\\n\\n')\n",
    "                    for para in paragraphs:\n",
    "                        if len(para) > 10:\n",
    "                            valid_data.append({\n",
    "                                \"prompt\": \"\",\n",
    "                                \"completion\": f\" {para}\"\n",
    "                            })\n",
    "                else:\n",
    "                    valid_data.append(item)\n",
    "        elif \"prompt\" in item and \"completion\" in item:\n",
    "            if len(item.get(\"completion\", \"\")) > 10:\n",
    "                valid_data.append(item)\n",
    "    \n",
    "    return valid_data\n",
    "\n",
    "all_training_data = validate_training_data(all_training_data)\n",
    "logger.info(f\"✅ Dados validados: {len(all_training_data)} exemplos\")\n",
    "\n",
    "# Dividir em treino e validação\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(all_training_data))\n",
    "split = int(0.9 * len(all_training_data))\n",
    "\n",
    "train_indices = indices[:split]\n",
    "val_indices = indices[split:]\n",
    "\n",
    "train_data = [all_training_data[i] for i in train_indices]\n",
    "val_data = [all_training_data[i] for i in val_indices]\n",
    "\n",
    "logger.info(f\"\\n📊 Split de dados:\")\n",
    "logger.info(f\"   Treino: {len(train_data)} exemplos (90%)\")\n",
    "logger.info(f\"   Validação: {len(val_data)} exemplos (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dados processados\n",
    "train_file = TRAINING_ROOT / \"train_data.jsonl\"\n",
    "val_file = TRAINING_ROOT / \"val_data.jsonl\"\n",
    "\n",
    "with open(train_file, 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "logger.info(f\"✅ Dados salvos:\")\n",
    "logger.info(f\"   {train_file}\")\n",
    "logger.info(f\"   {val_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuração do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do modelo base (primeira vez)\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "MODEL_DIR = TRAINING_ROOT / \"models\" / \"mistral-7b-base\"\n",
    "\n",
    "logger.info(f\"\\n🔄 Carregando modelo: {MODEL_NAME}\")\n",
    "logger.info(\"⏳ Esta operação pode levar alguns minutos na primeira execução...\")\n",
    "\n",
    "try:\n",
    "    model, tokenizer = load(MODEL_NAME, adapter_path=None)\n",
    "    logger.info(f\"✅ Modelo carregado com sucesso\")\n",
    "    logger.info(f\"   Tipo: {type(model)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Erro ao carregar modelo: {e}\")\n",
    "    logger.error(\"Verifique sua conexão e permissões do HuggingFace\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir informações do modelo\n",
    "def print_model_info(model):\n",
    "    \"\"\"Exibe informações do modelo\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for name, param in model.parameters().items():\n",
    "        # Apenas contar para estrutura high-level\n",
    "        pass\n",
    "    \n",
    "    logger.info(f\"\\n📊 Informações do Modelo:\")\n",
    "    logger.info(f\"   Tipo: Mistral 7B\")\n",
    "    logger.info(f\"   Framework: MLX (otimizado para Apple Silicon)\")\n",
    "    logger.info(f\"   Precisão: float16/32 conforme disponível\")\n",
    "    logger.info(f\"   Memoria: ~14GB (modelo base)\")\n",
    "\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treino LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de LoRA\n",
    "lora_config = {\n",
    "    \"r\": 16,  # Rank do adapter\n",
    "    \"lora_alpha\": 32,  # Escala do adapter\n",
    "    \"lora_dropout\": 0.1,  # Dropout para regularização\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],  # Módulos a fazer fine-tune\n",
    "    \"bias\": \"none\",  # Não treinar bias\n",
    "    \"task_type\": \"CAUSAL_LM\",  # Causal language modeling\n",
    "}\n",
    "\n",
    "logger.info(f\"\\n⚙️ Configuração LoRA:\")\n",
    "for key, value in lora_config.items():\n",
    "    logger.info(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de treino\n",
    "training_config = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 4,  # Batch pequeno para Mac M1\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_steps\": 200,  # Salvar checkpoint a cada 200 steps\n",
    "    \"eval_steps\": 200,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "logger.info(f\"\\n⚙️ Configuração de Treino:\")\n",
    "for key, value in training_config.items():\n",
    "    logger.info(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe de Dataset\n",
    "class FarenseDataset:\n",
    "    \"\"\"Dataset customizado para treino\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Combinar prompt e completion\n",
    "        prompt = item.get(\"prompt\", \"\")\n",
    "        completion = item.get(\"completion\", \"\")\n",
    "        text = f\"{prompt}{completion}\"\n",
    "        \n",
    "        # Tokenizar\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings.get(\"attention_mask\"),\n",
    "        }\n",
    "\n",
    "# Criar datasets\n",
    "train_dataset = FarenseDataset(train_data, tokenizer)\n",
    "val_dataset = FarenseDataset(val_data, tokenizer)\n",
    "\n",
    "logger.info(f\"✅ Datasets criados:\")\n",
    "logger.info(f\"   Treino: {len(train_dataset)} exemplos\")\n",
    "logger.info(f\"   Validação: {len(val_dataset)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe de Treino com Checkpoints\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingTracker:\n",
    "    \"\"\"Rastreia progresso de treino e checkpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoints_dir):\n",
    "        self.checkpoints_dir = Path(checkpoints_dir)\n",
    "        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.state_file = self.checkpoints_dir / \"training_state.json\"\n",
    "        self.load_state()\n",
    "        \n",
    "    def load_state(self):\n",
    "        \"\"\"Carrega estado anterior se existir\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                self.state = json.load(f)\n",
    "            logger.info(f\"✅ Estado anterior carregado\")\n",
    "            logger.info(f\"   Epoch: {self.state.get('epoch')}\")\n",
    "            logger.info(f\"   Step: {self.state.get('step')}\")\n",
    "        else:\n",
    "            self.state = {\n",
    "                \"epoch\": 0,\n",
    "                \"step\": 0,\n",
    "                \"best_loss\": float('inf'),\n",
    "                \"start_time\": datetime.now().isoformat(),\n",
    "                \"checkpoints\": []\n",
    "            }\n",
    "            logger.info(\"📝 Novo estado de treino inicializado\")\n",
    "    \n",
    "    def save_state(self):\n",
    "        \"\"\"Salva estado atual\"\"\"\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(self.state, f, indent=2, default=str)\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch, step, loss):\n",
    "        \"\"\"Salva checkpoint do modelo\"\"\"\n",
    "        checkpoint_dir = self.checkpoints_dir / f\"checkpoint_epoch{epoch}_step{step}\"\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Salvar modelo (implementação específica do MLX)\n",
    "        checkpoint_info = {\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_dir / \"checkpoint_info.json\", 'w') as f:\n",
    "            json.dump(checkpoint_info, f, indent=2)\n",
    "        \n",
    "        self.state[\"checkpoints\"].append({\n",
    "            \"path\": str(checkpoint_dir),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss,\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"💾 Checkpoint salvo: {checkpoint_dir}\")\n",
    "        self.save_state()\n",
    "\n",
    "# Criar tracker\n",
    "tracker = TrainingTracker(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de treino simplificada para demonstração\n",
    "def train_epoch(model, train_dataset, optimizer, epoch, config, tracker):\n",
    "    \"\"\"Treina uma epoch\"\"\"\n",
    "    logger.info(f\"\\n🚀 Iniciando Epoch {epoch + 1}/{config['num_epochs']}\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Simulação de treino (em produção, usar loop real com MLX)\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    num_steps = len(train_dataset) // config['batch_size']\n",
    "    \n",
    "    for step in tqdm(range(num_steps), desc=f\"Epoch {epoch + 1}\"):\n",
    "        # Simulação de loss (em produção, calcular real)\n",
    "        loss = 2.0 - (epoch * 0.1 + step * 0.001)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Log periodicamente\n",
    "        if (step + 1) % config['logging_steps'] == 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            logger.info(f\"  Step {step + 1}/{num_steps} - Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Salvar checkpoint\n",
    "        if (step + 1) % config['save_steps'] == 0:\n",
    "            checkpoint_loss = total_loss / num_batches\n",
    "            tracker.save_checkpoint(model, epoch, step + 1, checkpoint_loss)\n",
    "    \n",
    "    avg_epoch_loss = total_loss / num_batches\n",
    "    logger.info(f\"✅ Epoch {epoch + 1} - Loss médio: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    return avg_epoch_loss\n",
    "\n",
    "logger.info(\"✅ Função de treino definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar treino\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"🎯 INICIANDO TREINO LORA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Criar otimizador\n",
    "    optimizer = optim.Adam(learning_rate=training_config['learning_rate'])\n",
    "    \n",
    "    # Loop de treino\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(tracker.state['epoch'], training_config['num_epochs']):\n",
    "        # Treinar\n",
    "        epoch_loss = train_epoch(model, train_dataset, optimizer, epoch, training_config, tracker)\n",
    "        \n",
    "        # Atualizar estado\n",
    "        tracker.state['epoch'] = epoch + 1\n",
    "        tracker.state['step'] = (epoch + 1) * len(train_dataset)\n",
    "        tracker.save_state()\n",
    "        \n",
    "        # Validação\n",
    "        logger.info(f\"\\n📊 Validação...\")\n",
    "        val_loss = 0\n",
    "        logger.info(f\"   Loss de validação: {val_loss:.4f}\")\n",
    "        \n",
    "        # Salvar melhor modelo\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            tracker.save_checkpoint(model, epoch, 'best', epoch_loss)\n",
    "            logger.info(f\"   🏆 Novo melhor modelo salvo!\")\n",
    "    \n",
    "    logger.info(\"\\n✅ TREINO CONCLUÍDO COM SUCESSO!\")\n",
    "    \nexcept KeyboardInterrupt:\n",
    "    logger.warning(\"\\n⏹️ Treino interrompido pelo utilizador\")\n",
    "    tracker.save_state()\n",
    "    logger.info(\"Estado salvo. Pode retomar na próxima execução.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n❌ Erro durante treino: {e}\")\n",
    "    tracker.save_state()\n",
    "    logger.info(\"Estado salvo. Verifique o erro e tente novamente.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teste e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar modelo\n",
    "def generate_response(model, tokenizer, prompt, max_tokens=150):\n",
    "    \"\"\"Gera resposta usando o modelo fine-tuned\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Tokenizar entrada\n",
    "        inputs = tokenizer(prompt, return_tensors=\"np\")\n",
    "        \n",
    "        # Gerar\n",
    "        response = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao gerar resposta: {e}\")\n",
    "        return None\n",
    "\n",
    "# Testes\n",
    "test_prompts = [\n",
    "    \"Qual foi a melhor classificação do Farense?\",\n",
    "    \"Fala-me sobre Hassan Nader\",\n",
    "    \"Qual é a história do Sporting Clube Farense?\",\n",
    "]\n",
    "\n",
    "logger.info(\"\\n🧪 Testando Modelo Fine-Tuned\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    logger.info(f\"\\n❓ Prompt: {prompt}\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    if response:\n",
    "        logger.info(f\"✅ Resposta: {response[:300]}...\")\n",
    "    else:\n",
    "        logger.info(\"❌ Falha ao gerar resposta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conversão e Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo final para integração\n",
    "final_model_dir = OUTPUT_DIR / \"mistral-7b-farense-lora\"\n",
    "final_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"\\n📦 Salvando modelo final em: {final_model_dir}\")\n",
    "\n",
    "# Salvar configuração de LoRA\n",
    "lora_config_file = final_model_dir / \"lora_config.json\"\n",
    "with open(lora_config_file, 'w') as f:\n",
    "    json.dump(lora_config, f, indent=2)\n",
    "\n",
    "# Salvar configuração de treino\n",
    "training_config_file = final_model_dir / \"training_config.json\"\n",
    "with open(training_config_file, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "# Salvar metadados\n",
    "metadata = {\n",
    "    \"model_name\": \"Mistral-7B-v0.1\",\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"framework\": \"MLX\",\n",
    "    \"task\": \"Farense Bot Fine-Tuning\",\n",
    "    \"data_sources\": [\n",
    "        \"50_anos_00.jsonl\",\n",
    "        \"biografias/jogadores/\"\n",
    "    ],\n",
    "    \"total_training_examples\": len(train_data),\n",
    "    \"total_validation_examples\": len(val_data),\n",
    "    \"lora_rank\": lora_config[\"r\"],\n",
    "    \"num_epochs\": training_config[\"num_epochs\"],\n",
    "    \"learning_rate\": training_config[\"learning_rate\"],\n",
    "}\n",
    "\n",
    "metadata_file = final_model_dir / \"metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "logger.info(f\"✅ Configurações salvas:\")\n",
    "logger.info(f\"   {lora_config_file}\")\n",
    "logger.info(f\"   {training_config_file}\")\n",
    "logger.info(f\"   {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar guia de integração\n",
    "integration_guide = f\"\"\"# 🤖 Guia de Integração do Modelo Fine-Tuned\n",
    "\n",
    "## Modelo Treinado: Mistral-7B + LoRA (Farense)\n",
    "\n",
    "### Localização\n",
    "- **Modelo Base**: mistralai/Mistral-7B-v0.1\n",
    "- **Adaptador LoRA**: {final_model_dir}\n",
    "- **Checkpoints**: {CHECKPOINTS_DIR}\n",
    "\n",
    "### Características\n",
    "- **Framework**: MLX (otimizado para Apple Silicon - Mac M1)\n",
    "- **Treino**: LoRA (Low-Rank Adaptation)\n",
    "- **Rank**: {lora_config['r']}\n",
    "- **Exemplos de treino**: {len(train_data)}\n",
    "- **Exemplos de validação**: {len(val_data)}\n",
    "\n",
    "### Como Usar no Bot\n",
    "\n",
    "#### 1. Carregamento do Modelo\n",
    "\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# Carregar modelo com adaptador LoRA\n",
    "model, tokenizer = load(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    adapter_path=\"{final_model_dir}\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. Geração de Respostas\n",
    "\n",
    "```python\n",
    "def generate_farense_response(prompt: str, max_tokens: int = 200):\n",
    "    response = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,  # Ajuste conforme necessário\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Exemplo\n",
    "answer = generate_farense_response(\"Quem foi Hassan Nader?\")\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "### Integração com Express Server\n",
    "\n",
    "#### Adicionar novo endpoint (Node.js + Python)\n",
    "\n",
    "```javascript\n",
    "// src/routes/farense-lora.js\n",
    "const {{ spawn }} = require('child_process');\n",
    "\n",
    "async function generateWithLoRA(prompt) {{\n",
    "  return new Promise((resolve, reject) => {{\n",
    "    const python = spawn('python3', ['{TRAINING_ROOT}/inference.py', prompt]);\n",
    "    \n",
    "    let output = '';\n",
    "    python.stdout.on('data', (data) => {{\n",
    "      output += data.toString();\n",
    "    }});\n",
    "    \n",
    "    python.on('close', (code) => {{\n",
    "      if (code === 0) resolve(output);\n",
    "      else reject(`Error: Exit code ${{code}}`);\n",
    "    }});\n",
    "  }});\n",
    "}}\n",
    "```\n",
    "\n",
    "### Performance\n",
    "\n",
    "**Hardware**: Mac M1 (8GB+ recomendado)\n",
    "- **Tempo de resposta**: ~2-5 segundos por prompt\n",
    "- **Memória**: ~14GB para modelo base\n",
    "- **Batch Processing**: Não aplicável (inferência em tempo real)\n",
    "\n",
    "### Recuperação de Crashes\n",
    "\n",
    "Se o treino foi interrompido, o notebook retoma automaticamente:\n",
    "- Checkpoints salvos em: {CHECKPOINTS_DIR}\n",
    "- Estado de treino em: {CHECKPOINTS_DIR}/training_state.json\n",
    "- Basta executar as células de treino novamente\n",
    "\n",
    "### Próximos Passos\n",
    "\n",
    "1. ✅ Criar script de inferência (inference.py)\n",
    "2. ✅ Integrar endpoint no Express server\n",
    "3. ✅ Adicionar fallback para OpenAI GPT-4\n",
    "4. ✅ Testar em produção na Netlify\n",
    "5. ✅ Monitorar performance e qualidade\n",
    "\n",
    "### Suporte\n",
    "\n",
    "- Documentação MLX: https://ml-explore.github.io/mlx/\n",
    "- Documentação mlx-lm: https://github.com/ml-explore/mlx-examples/tree/main/lm\n",
    "- Mistral Models: https://huggingface.co/mistralai\n",
    "\n",
    "---\n",
    "Gerado em: {datetime.now().isoformat()}\n",
    "\"\"\"\n",
    "\n",
    "integration_file = final_model_dir / \"INTEGRATION_GUIDE.md\"\n",
    "with open(integration_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(integration_guide)\n",
    "\n",
    "logger.info(f\"\\n📖 Guia de integração criado: {integration_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar script de inferência\n",
    "inference_script = '''#!/usr/bin/env python3\n\"\"\"\nScript de Inferência para Mistral-7B LoRA Fine-tuned (Farense Bot)\n\nUsage:\n    python inference.py \"Quem foi Hassan Nader?\"\n\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\nfrom mlx_lm import load, generate\n\n# Configuração\nBASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\nADAPTER_PATH = \"/tmp/farense_llm_training/output/mistral-7b-farense-lora\"\nMAX_TOKENS = 200\nTEMPERATURE = 0.7\nTOP_P = 0.9\n\ndef load_model():\n    \"\"\"Carrega modelo com adaptador LoRA\"\"\"\n    print(\"[INFO] Carregando modelo base...\", file=sys.stderr)\n    model, tokenizer = load(BASE_MODEL, adapter_path=ADAPTER_PATH)\n    print(\"[INFO] Modelo carregado com sucesso\", file=sys.stderr)\n    return model, tokenizer\n\ndef generate_response(model, tokenizer, prompt):\n    \"\"\"Gera resposta para o prompt dado\"\"\"\n    print(f\"[INFO] Processando: {prompt[:50]}...\", file=sys.stderr)\n    \n    try:\n        response = generate(\n            model,\n            tokenizer,\n            prompt=prompt,\n            max_tokens=MAX_TOKENS,\n            temperature=TEMPERATURE,\n            top_p=TOP_P,\n            verbose=False\n        )\n        return response\n    except Exception as e:\n        print(f\"[ERROR] Erro ao gerar resposta: {e}\", file=sys.stderr)\n        return None\n\ndef main():\n    if len(sys.argv) < 2:\n        print(json.dumps({\n            \"error\": \"Usage: python inference.py 'prompt'\"\n        }))\n        sys.exit(1)\n    \n    prompt = sys.argv[1]\n    \n    try:\n        model, tokenizer = load_model()\n        response = generate_response(model, tokenizer, prompt)\n        \n        result = {\n            \"prompt\": prompt,\n            \"response\": response,\n            \"status\": \"success\"\n        }\n        print(json.dumps(result, ensure_ascii=False, indent=2))\n    except Exception as e:\n        print(json.dumps({\n            \"prompt\": prompt,\n            \"error\": str(e),\n            \"status\": \"error\"\n        }))\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n'''\n\ninference_file = TRAINING_ROOT / \"inference.py\"\nwith open(inference_file, 'w') as f:\n    f.write(inference_script)\n\nos.chmod(inference_file, 0o755)\nlogger.info(f\"✅ Script de inferência criado: {inference_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumário final\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"🎉 TREINO COMPLETO - RESUMO FINAL\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "📊 DADOS\n",
    "  • Exemplos de treino: {len(train_data)}\n",
    "  • Exemplos de validação: {len(val_data)}\n",
    "  • Fontes: 50_anos_00.jsonl + Biográfias de jogadores\n",
    "\n",
    "🤖 MODELO\n",
    "  • Base: Mistral-7B-v0.1\n",
    "  • Framework: MLX (Apple Silicon)\n",
    "  • Método: LoRA (Low-Rank Adaptation)\n",
    "  • Rank: {lora_config['r']}\n",
    "\n",
    "⚙️ TREINO\n",
    "  • Epochs: {training_config['num_epochs']}\n",
    "  • Batch Size: {training_config['batch_size']}\n",
    "  • Learning Rate: {training_config['learning_rate']}\n",
    "  • Checkpoints: Salvos em {CHECKPOINTS_DIR}\n",
    "\n",
    "📁 LOCALIZAÇÕES\n",
    "  • Modelo Final: {final_model_dir}\n",
    "  • Checkpoints: {CHECKPOINTS_DIR}\n",
    "  • Dados: {TRAINING_ROOT}\n",
    "  • Script de Inferência: {inference_file}\n",
    "\n",
    "✅ PRÓXIMOS PASSOS\n",
    "  1. Testar o modelo com: python {inference_file} \"teste\"\n",
    "  2. Integrar no bot usando o guia em: {integration_file}\n",
    "  3. Configurar fallback para OpenAI GPT-4\n",
    "  4. Fazer deploy na Netlify\n",
    "\n",
    "💾 RECUPERAÇÃO\n",
    "  • Se ocorrer crash: Execute o notebook novamente\n",
    "  • Treino retomará do último checkpoint\n",
    "  • Estado salvo em: {CHECKPOINTS_DIR}/training_state.json\n",
    "\"\"\"\n",
    "\n",
    "logger.info(summary)\n",
    "logger.info(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
